{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLD4kgZd9OZ1",
        "outputId": "b27d9b1a-c663-47e0-e167-3954c401c369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f78d14959a8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from facenet_pytorch import MTCNN\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# Step 1: Face Detection Branch\n",
        "class FaceDetectionBranch:\n",
        "    def __init__(self, image_size=160, margin=20, min_face_size=20, thresholds=[0.6, 0.7, 0.7], factor=0.709):\n",
        "        \"\"\"\n",
        "        Initialize the face detection branch with MTCNN\n",
        "        \"\"\"\n",
        "        # Check if MPS is available (Apple Silicon)\n",
        "        self.device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize MTCNN\n",
        "        self.mtcnn = MTCNN(\n",
        "            image_size=image_size,\n",
        "            margin=margin,\n",
        "            min_face_size=min_face_size,\n",
        "            thresholds=thresholds,\n",
        "            factor=factor,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "    def detect_faces(self, image):\n",
        "        \"\"\"\n",
        "        Detect faces in the input image\n",
        "        \"\"\"\n",
        "        # Convert BGR to RGB (MTCNN expects RGB)\n",
        "        if isinstance(image, np.ndarray) and image.shape[-1] == 3:\n",
        "            # Check if image is already RGB\n",
        "            if image.dtype == np.uint8 and image.shape[2] == 3:\n",
        "                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            else:\n",
        "                rgb_image = image\n",
        "        else:\n",
        "            # Convert PIL Image to numpy array if needed\n",
        "            if isinstance(image, Image.Image):\n",
        "                rgb_image = np.array(image)\n",
        "            else:\n",
        "                rgb_image = image\n",
        "\n",
        "        # Detect faces\n",
        "        boxes, probs = self.mtcnn.detect(rgb_image)\n",
        "\n",
        "        # Convert to [x, y, width, height] format\n",
        "        faces = []\n",
        "        if boxes is not None:\n",
        "            for box in boxes:\n",
        "                x, y, x2, y2 = box\n",
        "                width = x2 - x\n",
        "                height = y2 - y\n",
        "                faces.append([int(x), int(y), int(width), int(height)])\n",
        "\n",
        "        return faces\n",
        "\n",
        "    def crop_faces(self, image, faces, padding=0.2):\n",
        "        \"\"\"\n",
        "        Crop detected faces from the original image\n",
        "        \"\"\"\n",
        "        # Convert PIL Image to numpy array if needed\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        # If image is RGB, convert to BGR for OpenCV processing\n",
        "        if image.shape[-1] == 3 and image.dtype == np.uint8:\n",
        "            image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        else:\n",
        "            image_bgr = image\n",
        "\n",
        "        height, width = image_bgr.shape[:2]\n",
        "        cropped_faces = []\n",
        "\n",
        "        for face in faces:\n",
        "            x, y, w, h = face\n",
        "\n",
        "            # Add padding\n",
        "            pad_w = int(w * padding)\n",
        "            pad_h = int(h * padding)\n",
        "\n",
        "            # Calculate padded coordinates with boundary checks\n",
        "            x1 = max(0, x - pad_w)\n",
        "            y1 = max(0, y - pad_h)\n",
        "            x2 = min(width, x + w + pad_w)\n",
        "            y2 = min(height, y + h + pad_h)\n",
        "\n",
        "            # Crop face\n",
        "            face_image = image_bgr[y1:y2, x1:x2]\n",
        "\n",
        "            # Only add if we have a valid crop\n",
        "            if face_image.size > 0:\n",
        "                cropped_faces.append(face_image)\n",
        "\n",
        "        return cropped_faces\n",
        "\n",
        "# Step 2: Skin Tone Extractor\n",
        "class SkinToneExtractor:\n",
        "    def __init__(self, n_clusters=4):\n",
        "        \"\"\"\n",
        "        Initialize the skin tone extractor\n",
        "        \"\"\"\n",
        "        self.n_clusters = n_clusters\n",
        "\n",
        "        # Define skin tone ranges in HSV color space\n",
        "        # These are general ranges that work well for most skin tones\n",
        "        self.lower_hsv = np.array([0, 20, 70], dtype=np.uint8)\n",
        "        self.upper_hsv = np.array([25, 255, 255], dtype=np.uint8)\n",
        "\n",
        "    def extract_skin_mask(self, image):\n",
        "        \"\"\"\n",
        "        Extract skin regions from the face image\n",
        "        \"\"\"\n",
        "        # Convert to HSV color space\n",
        "        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # Create skin mask using HSV thresholds\n",
        "        skin_mask = cv2.inRange(hsv_image, self.lower_hsv, self.upper_hsv)\n",
        "\n",
        "        # Apply morphological operations to clean up the mask\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "        skin_mask = cv2.erode(skin_mask, kernel, iterations=1)\n",
        "        skin_mask = cv2.dilate(skin_mask, kernel, iterations=1)\n",
        "\n",
        "        # Apply Gaussian blur to smooth the mask\n",
        "        skin_mask = cv2.GaussianBlur(skin_mask, (3, 3), 0)\n",
        "\n",
        "        return skin_mask\n",
        "\n",
        "    def find_dominant_color(self, image, mask):\n",
        "        \"\"\"\n",
        "        Find the dominant color in the skin regions\n",
        "        \"\"\"\n",
        "        # Apply mask to the original image\n",
        "        skin_only = cv2.bitwise_and(image, image, mask=mask)\n",
        "\n",
        "        # Reshape the image to be a list of pixels (only non-zero values)\n",
        "        pixels = skin_only.reshape(-1, 3)\n",
        "        pixels = pixels[np.all(pixels != [0, 0, 0], axis=1)]\n",
        "\n",
        "        # If no skin pixels are found, return a default value\n",
        "        if len(pixels) == 0:\n",
        "            return np.array([211, 169, 150])  # Default skin tone\n",
        "\n",
        "        # Cluster the skin pixels\n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=10)\n",
        "        kmeans.fit(pixels)\n",
        "\n",
        "        # Get the most common cluster\n",
        "        counts = Counter(kmeans.labels_)\n",
        "        center_idx = counts.most_common(1)[0][0]\n",
        "        dominant_color = kmeans.cluster_centers_[center_idx].astype(int)\n",
        "\n",
        "        return dominant_color\n",
        "\n",
        "    def extract_skin_tone(self, face_image):\n",
        "        \"\"\"\n",
        "        Extract the skin tone from a face image\n",
        "        \"\"\"\n",
        "        # If face_image is a list with one element, extract the element\n",
        "        if isinstance(face_image, list) and len(face_image) == 1:\n",
        "            face_image = face_image[0]\n",
        "\n",
        "        # Extract skin mask\n",
        "        skin_mask = self.extract_skin_mask(face_image)\n",
        "\n",
        "        # Find dominant color\n",
        "        dominant_color = self.find_dominant_color(face_image, skin_mask)\n",
        "\n",
        "        # Convert BGR to RGB\n",
        "        rgb_color = dominant_color[::-1]\n",
        "\n",
        "        # Convert RGB to hex code\n",
        "        hex_code = '#{:02x}{:02x}{:02x}'.format(rgb_color[0], rgb_color[1], rgb_color[2])\n",
        "\n",
        "        return hex_code, rgb_color\n",
        "\n",
        "# Function for Fitzpatrick scale classification\n",
        "def classify_skin_tone(rgb_color):\n",
        "    \"\"\"\n",
        "    Classify skin tone according to Fitzpatrick scale\n",
        "    \"\"\"\n",
        "    # Calculate Individual Typology Angle (ITA)\n",
        "    # ITA = arctan((L* - 50) / b*) * 180/π\n",
        "    # Convert RGB to L*a*b*\n",
        "    rgb_normalized = np.array([[rgb_color]], dtype=np.float32) / 255.0\n",
        "    bgr_normalized = rgb_normalized[:, :, ::-1]  # RGB to BGR\n",
        "    lab_color = cv2.cvtColor(bgr_normalized, cv2.COLOR_BGR2Lab)[0][0]\n",
        "\n",
        "    L = lab_color[0]\n",
        "    b = lab_color[2] - 128  # Adjust b* value (b* in OpenCV is from 0-255, center at 128)\n",
        "\n",
        "    # Calculate ITA\n",
        "    if b == 0:\n",
        "        b = 0.01  # Avoid division by zero\n",
        "    ita = np.arctan((L - 50) / b) * 180 / np.pi\n",
        "\n",
        "    # Classify based on ITA values\n",
        "    if ita > 55:\n",
        "        return \"Type I - Very fair, always burns\"\n",
        "    elif 48 <= ita <= 55:\n",
        "        return \"Type II - Fair, usually burns\"\n",
        "    elif 41 <= ita < 48:\n",
        "        return \"Type III - Medium, sometimes burns\"\n",
        "    elif 30 <= ita < 41:\n",
        "        return \"Type IV - Olive, rarely burns\"\n",
        "    elif 19 <= ita < 30:\n",
        "        return \"Type V - Brown, very rarely burns\"\n",
        "    else:\n",
        "        return \"Type VI - Dark brown to black, never burns\"\n",
        "\n",
        "# Palette recommendation\n",
        "def predict_palette(hex_code):\n",
        "    \"\"\"\n",
        "    Generate a color palette based on the skin tone hex code\n",
        "    Returns 5 complementary colors as hex codes\n",
        "    \"\"\"\n",
        "    # Define color palettes for different skin tone ranges\n",
        "    light_palette = [\"#F4C2C2\", \"#AEC6CF\", \"#E6E6FA\", \"#FFFFF0\", \"#FFDB58\"]\n",
        "    medium_palette = [\"#D2B48C\", \"#FFDB58\", \"#E2725B\", \"#778899\", \"#B0C4DE\"]\n",
        "    dark_palette = [\"#FFD700\", \"#00A86B\", \"#DC143C\", \"#4169E1\", \"#800080\"]\n",
        "\n",
        "    # Extract RGB values from hex\n",
        "    hex_code = hex_code.lstrip('#')\n",
        "    r, g, b = tuple(int(hex_code[i:i+2], 16) for i in (0, 2, 4))\n",
        "\n",
        "    # Simple classification based on brightness\n",
        "    brightness = (r * 299 + g * 587 + b * 114) / 1000\n",
        "\n",
        "    if brightness > 170:\n",
        "        return light_palette\n",
        "    elif brightness > 100:\n",
        "        return medium_palette\n",
        "    else:\n",
        "        return dark_palette\n",
        "\n",
        "# Detect faces and extract skin tone\n",
        "def process_face_image(image):\n",
        "    \"\"\"\n",
        "    Process an image to detect faces and extract skin tone\n",
        "    Returns skin tone hex code, palette, and fitzpatrick type\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        return None, None, None\n",
        "\n",
        "    # Initialize face detection\n",
        "    face_detector = FaceDetectionBranch()\n",
        "\n",
        "    # Convert PIL Image to numpy if needed\n",
        "    img_for_detection = np.array(image) if isinstance(image, Image.Image) else image\n",
        "\n",
        "    # Detect faces\n",
        "    faces = face_detector.detect_faces(img_for_detection)\n",
        "\n",
        "    # If no faces detected, return None\n",
        "    if not faces:\n",
        "        return None, None, None\n",
        "\n",
        "    # Crop faces\n",
        "    cropped_faces = face_detector.crop_faces(img_for_detection, faces)\n",
        "\n",
        "    if not cropped_faces:\n",
        "        return None, None, None\n",
        "\n",
        "    # Initialize skin tone extractor\n",
        "    skin_extractor = SkinToneExtractor()\n",
        "\n",
        "    # Extract skin tone from first face\n",
        "    hex_code, rgb_color = skin_extractor.extract_skin_tone(cropped_faces[0])\n",
        "\n",
        "    # Classify skin tone\n",
        "    fitzpatrick_type = classify_skin_tone(rgb_color)\n",
        "\n",
        "    # Generate palette\n",
        "    palette = predict_palette(hex_code)\n",
        "\n",
        "    return hex_code, palette, fitzpatrick_type\n",
        "\n",
        "# Display results with HTML\n",
        "def display_output(image):\n",
        "    if image is None:\n",
        "        return \"<p>Please upload an image first.</p>\", \"<p>No palette available.</p>\"\n",
        "\n",
        "    # Process the image\n",
        "    hex_code, palette, fitzpatrick_type = process_face_image(image)\n",
        "\n",
        "    if hex_code is None:\n",
        "        return \"<p>No face detected in the image. Please try another image.</p>\", \"<p>No palette available.</p>\"\n",
        "\n",
        "    # Create HTML for skin tone display\n",
        "    tone_section = f\"\"\"\n",
        "    <div style='text-align:center; padding: 10px;'>\n",
        "        <h3 style='text-align:center;'>Detected Skin Tone</h3>\n",
        "        <div style='display:flex; justify-content:center; align-items:center; gap:20px;'>\n",
        "            <div style='width:80px; height:80px; background:{hex_code};\n",
        "                border-radius:8px; box-shadow:0 1px 4px rgba(0,0,0,0.1);'></div>\n",
        "            <div>\n",
        "                <h2 style='color:#333; margin:0;'>{hex_code}</h2>\n",
        "                <p style='margin:5px 0 0 0;'>{fitzpatrick_type}</p>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Create HTML for palette display\n",
        "    swatches = \"<div style='display:flex; gap:12px; justify-content:center; margin-top:10px;'>\"\n",
        "    for color in palette:\n",
        "        swatches += f\"<div style='width:60px; height:60px; background:{color}; border-radius:8px; box-shadow:0 1px 4px rgba(0,0,0,0.1);'></div>\"\n",
        "    swatches += \"</div>\"\n",
        "\n",
        "    palette_section = f\"\"\"\n",
        "    <div style='padding: 10px;'>\n",
        "        <h3 style='text-align:center;'>Recommended Color Palette</h3>\n",
        "        {swatches}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    return tone_section, palette_section\n",
        "\n",
        "# Custom CSS\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "    background: #fdf6f0;\n",
        "    font-family: 'Helvetica Neue', sans-serif;\n",
        "    color: #222;\n",
        "}\n",
        "#upload-btn {\n",
        "    margin: auto;\n",
        "    border: 2px dashed #ccc;\n",
        "    padding: 16px;\n",
        "    background: white;\n",
        "    box-shadow: 0 4px 12px rgba(0,0,0,0.05);\n",
        "    border-radius: 10px;\n",
        "}\n",
        ".gr-button {\n",
        "    background-color: #FFBC80;\n",
        "    color: #222;\n",
        "    font-weight: bold;\n",
        "    border: none;\n",
        "}\n",
        ".gr-button:hover {\n",
        "    background-color: #ffaa55;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create Gradio Interface\n",
        "with gr.Blocks(css=custom_css, title=\"SkinShade AI\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "        <div style='text-align:center; max-width: 640px; margin: auto; padding-top: 40px;'>\n",
        "            <h1 style='font-size: 32px;'>Skin Tone & Color Palette Recommender</h1>\n",
        "            <p style='font-size: 16px; color: #444;'>\n",
        "                Upload a photo to find your skin tone and get personalized color suggestions.\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    image_input = gr.Image(type=\"pil\", label=\"Upload Your Photo\", elem_id=\"upload-btn\")\n",
        "    tone_output = gr.HTML()\n",
        "    palette_output = gr.HTML()\n",
        "    btn = gr.Button(\"Detect Tone\")\n",
        "\n",
        "    btn.click(display_output, inputs=image_input, outputs=[tone_output, palette_output])\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "        <div style='text-align:center; margin-top: 40px; color: #666; font-size: 14px;'>\n",
        "            We don't store any images. Everything runs locally during the session.\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)"
      ]
    }
  ]
}